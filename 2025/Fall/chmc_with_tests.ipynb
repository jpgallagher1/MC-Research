{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Description:\n",
    "    CHMC Implementation with AVF: FPI\n",
    "    USE THE CORRECT ENVIRONMENT:  CHMC_FALL_2025\n",
    "    YYYY-MM-DD\n",
    "    \n",
    "Author: John Gallagher\n",
    "Created: 2025-09-28\n",
    "Last Modified: 2025-10-09\n",
    "Version: 1.0.0\n",
    "\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import jit\n",
    "from functools import partial\n",
    "import time\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "@jit\n",
    "def gauss_ndimf_jax(x, precision_matrix = None, cov=None, dim = 2):\n",
    "    \"\"\"n-Dim Gaussian target distribution.\"\"\"\n",
    "    dim = len(x)\n",
    "    # Error Classes\n",
    "    class MultipleMatrices(Exception):\n",
    "        pass\n",
    "    # dealing with getting a precision matrix or cov matrix\n",
    "    if precision_matrix is not None and cov is not None:\n",
    "        raise MultipleMatrices(\"Please supply either a Precision Matrix or a Covariance Matrix\")\n",
    "    if precision_matrix is None and cov is not None:\n",
    "        precision_matrix = jnp.linalg.inv(cov)\n",
    "    if precision_matrix is None and cov is None:\n",
    "        precision_matrix = jnp.eye(dim)\n",
    "        # jnp.linalg.det(precision_matrix)**(-1/2)\n",
    "        # (2*jnp.pi)**(-dim/2)*\n",
    "    return jnp.exp(-x@precision_matrix@x)\n",
    "\n",
    "@jit\n",
    "def qex(qp):\n",
    "  return qp[:dim]\n",
    "@jit\n",
    "def pex(qp):\n",
    "  return qp[dim:]\n",
    "\n",
    "@jit\n",
    "def draw_p(qp, key):\n",
    "    q = qex(qp)\n",
    "    p = jax.random.normal(key,shape = (dim,))\n",
    "    return jnp.concatenate([q,p]), None\n",
    "@jit\n",
    "def leapfrog(qp):\n",
    "    def lf_step(carry_in, _):\n",
    "        q, p = qex(carry_in), pex(carry_in)\n",
    "        q_half = q + 0.5 * tau * gradH_p(q, p)\n",
    "        p_new = p - tau * gradH_q(q_half, p)\n",
    "        q_new = q_half + 0.5 * tau * gradH_p(q_half, p_new)\n",
    "        carry_out = jnp.concatenate([q_new, p_new])\n",
    "        return carry_out, _\n",
    "    qp_final,  _ = jax.lax.scan(lf_step, qp, xs=None, length=T)\n",
    "    return qp_final\n",
    "\n",
    "@jit\n",
    "def midpointFPI(qp):\n",
    "    x0 = qp\n",
    "    def G(y):\n",
    "        midpoint = 0.5*(x0+y)\n",
    "        return x0 + tau*J_H(grad_xH(midpoint))\n",
    "\n",
    "    def F(y):\n",
    "        return y-G(y)\n",
    "\n",
    "    def newton_step(qp):\n",
    "        jacF = jax.jacobian(F)(qp)\n",
    "        qpout = x0 - jnp.linalg.solve(jacF, F(qp))\n",
    "        return qpout\n",
    "\n",
    "    def cond(carry):\n",
    "        i, qp = carry\n",
    "        F_qp = F(qp)\n",
    "        err = jnp.linalg.norm(F_qp)\n",
    "        return (err > tol) & (i < max_iter)\n",
    "\n",
    "    def body_step(carry):\n",
    "        i, qp = carry\n",
    "        return [i + 1, newton_step(qp)]\n",
    "\n",
    "    _, qp_out = jax.lax.while_loop(cond, body_step, [0, qp])\n",
    "    return qp_out\n",
    "\n",
    "@jit\n",
    "def accept(delta, key):\n",
    "    alpha = jnp.minimum(1., jnp.exp(delta))\n",
    "    u = jax.random.uniform(key, shape=())\n",
    "    return u <= alpha\n",
    "@jit\n",
    "def hmc_kernel(carry, key):\n",
    "    qp0, _ = draw_p(carry, key)\n",
    "    qp_star = jit_integrator(qp0)\n",
    "    deltaH = jit_H(qp_star) - jit_H(qp0)\n",
    "    is_accepted = accept(deltaH, key)\n",
    "    qp_out = jnp.where(is_accepted, qp_star, qp0)\n",
    "    return qp_out, qp_out\n",
    "@jit\n",
    "def hmc_sampler(initial_sample, keys):\n",
    "    _, samples = jax.lax.scan(hmc_kernel, initial_sample, xs=keys)\n",
    "    return samples\n",
    "\n",
    "\n",
    "# Function handles into mechanics of HMC Sampler:\n",
    "def hamiltionian(q,p):\n",
    "    return 0.5 * (p@Mass_inv@p) - jnp.log(target(q))\n",
    "def xhamiltionian(qp):\n",
    "    q, p = qex(qp), pex(qp)\n",
    "    return 0.5 * (p@Mass_inv@p) - jnp.log(target(q))\n",
    "\n",
    "def J_H(gH):\n",
    "    \"\"\"Same operation as Symplectic Jacobian\"\"\"\n",
    "    return jnp.concatenate([gH[dim:],-gH[:dim]])\n",
    "\n",
    "# def J_simplec(n):\n",
    "#     zero_zero = jnp.zeros((n,n))\n",
    "#     zero_one = jnp.eye((n))\n",
    "#     one_zero = -jnp.eye((n))\n",
    "#     one_one = zero_zero\n",
    "#     return jnp.block([[zero_zero, zero_one],[one_zero, one_one]])\n",
    "# I don't know how jit syntax works yet so I just directly did it here.\n",
    "target = jit(gauss_ndimf_jax)\n",
    "grad_target = jit(jax.grad(target))\n",
    "gradH_p = jit(jax.grad(hamiltionian, argnums=1))\n",
    "gradH_q = jit(jax.grad(hamiltionian, argnums=0))\n",
    "jit_H = jit(xhamiltionian)\n",
    "grad_xH = jax.jit(jax.grad(xhamiltionian))\n",
    "# jit_target = jax.jit(target)\n",
    "# jit_grad_target = jax.jit(grad_target)\n",
    "# jit_gradH_p = jax.jit(gradH_p)\n",
    "# jit_gradH_q = jax.jit(gradH_q)\n",
    "# jit_integrator = jax.jit(leapfrog)\n",
    "# jit_integrator = jit(midpointFPI)\n",
    "\n",
    "# Set parameters\n",
    "key = jax.random.PRNGKey(1)\n",
    "dim = 3\n",
    "num_samples = 1\n",
    "mainnum_samples = 10000\n",
    "keys_start = jax.random.split(key, num_samples)\n",
    "keys_main = jax.random.split(key, mainnum_samples)\n",
    "qp_init = jax.random.normal(key, shape=(2*dim,))\n",
    "Mass_inv = jnp.eye(dim)\n",
    "tau = 0.2\n",
    "T = 1\n",
    "tol = 1e-4\n",
    "max_iter = 1000\n",
    "# compile\n",
    "start=time.time()\n",
    "sample_LF = hmc_sampler(qp_init, keys_start)\n",
    "end = time.time()\n",
    "print(\"1st run:\", end-start)\n",
    "# main run\n",
    "start = time.time()\n",
    "sample_LF = hmc_sampler(qp_init, keys_main).block_until_ready()\n",
    "end = time.time()\n",
    "print(f\"{mainnum_samples} runs: {end - start:.2f} \\n 1 run:  {(end-start)/mainnum_samples}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let H = 0.5*9(p.T@p) + 0.5*(q.T@Sigma_inv@q)\n",
    "# F(y) = y- x - tau*J@gradH(0.5*(y+x))\n",
    "# J_F(y) = I - 0.5*tau*J@JacH(0.5*(y+x))\n",
    "\n",
    "J_F = jnp.eye(2*dim) + jnp.block([[jnp.zeros(dim),0.5*tau*jnp.eye(dim)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_LF[:,:dim].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jit_integrator = jit(midpointFPI)\n",
    "# compile\n",
    "start=time.time()\n",
    "sample_FPI = hmc_sampler(qp_init, keys_start)\n",
    "end = time.time()\n",
    "print(\"1st run:\", end-start)\n",
    "# main run\n",
    "start = time.time()\n",
    "sample_FPI = hmc_sampler(qp_init, keys_main)\n",
    "end = time.time()\n",
    "print(f\"{mainnum_samples} runs: {end - start} \\n 1 run:  {(end-start)/mainnum_samples}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of unique samples for Leapfrog\n",
    "unique_samples_LF = jnp.unique(sample_LF[:,:dim], axis=0)\n",
    "num_unique_LF = unique_samples_LF.shape[0]\n",
    "print(f\"Number of unique samples (Leapfrog): {num_unique_LF}\")\n",
    "\n",
    "# Calculate the number of unique samples for Midpoint FPI\n",
    "unique_samples_FPI = jnp.unique(sample_FPI[:,:dim], axis=0)\n",
    "num_unique_FPI = unique_samples_FPI.shape[0]\n",
    "print(f\"Number of unique samples (Midpoint FPI): {num_unique_FPI}\")\n",
    "\n",
    "# Compare the number of unique samples to the total number of samples\n",
    "print(f\"Total number of samples: {mainnum_samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_FPI.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jit_integrator = jit(leapfrog)\n",
    "# # draw qp states\n",
    "# inits, _ = jax.vmap(draw_p, in_axes=(None, 0))(qp_init, keys_main)\n",
    "# vmap_integrator = jax.vmap(jit_integrator)\n",
    "# sample_LF = vmap_integrator(inits)\n",
    "\n",
    "# # Midpoint FPI\n",
    "# jit_integrator = jit(midpointFPI)\n",
    "# vmap_integrator = jax.vmap(jit_integrator)\n",
    "# sample_FPI = vmap_integrator(inits)\n",
    "\n",
    "sample_diff = sample_FPI - sample_LF\n",
    "\n",
    "# norm of the difference along row\n",
    "norm_diff = jnp.linalg.norm(sample_diff, axis=1)\n",
    "\n",
    "# mean,max,min\n",
    "print(\"Mean norm difference:\", jnp.mean(norm_diff))\n",
    "print(\"Max norm difference:\", jnp.max(norm_diff))\n",
    "print(\"Min norm difference:\", jnp.min(norm_diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "ax1.hist2d(np.array(sample_FPI[:,0]),np.array(sample_FPI[:,1]), bins=50, density = True)\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_xlabel('x-Value')\n",
    "ax1.set_title('FPI Resulting Accepted Distribution vs Target Function')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gauss_f(x):\n",
    "    \"\"\"Gaussian target distribution.\"\"\"\n",
    "    return np.exp(-x**2)\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "# Plot histogram of accepted samples\n",
    "ax1.hist(np.array(sample_FPI[:,1]), bins=50, density = True)\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_xlabel('x-Value')\n",
    "ax1.set_title('FPI Resulting Accepted Distribution vs Target Function')\n",
    "\n",
    "# Plot target distribution\n",
    "# ax2 = ax1.twinx()\n",
    "x_vals = np.linspace(-3, 3, 10000)\n",
    "y_vals = [gauss_f(x)/(np.pi)**0.5 for x in x_vals]\n",
    "ax1.plot(x_vals, y_vals, 'r-', label='Target Function')\n",
    "ax1.set_ylabel('Target Function Value')\n",
    "ax1.set_ylim(0, 0.6)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "# Plot histogram of accepted samples\n",
    "ax1.hist2d(np.array(sample_LF[:,0]),np.array(sample_LF[:,1]), bins=50, density = True)\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_xlabel('x-Value')\n",
    "ax1.set_title('FPI Resulting Accepted Distribution vs Target Function')\n",
    "\n",
    "# Plot target distribution\n",
    "# # ax2 = ax1.twinx()\n",
    "# x_vals = np.linspace(-3, 3, 10000)\n",
    "# y_vals = [gauss_f(x)/(np.pi)**0.5 for x in x_vals]\n",
    "# ax1.plot(x_vals, y_vals, 'r-', label='Target Function')\n",
    "# ax1.set_ylabel('Target Function Value')\n",
    "# ax1.set_ylim(0, 0.6)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_diff = sample_FPI[:,:3] - sample_LF[:,:3]\n",
    "\n",
    "# norm of the difference along row\n",
    "norm_diff = jnp.linalg.norm(sample_diff, axis=1)\n",
    "\n",
    "# mean,max,min\n",
    "print(\"Mean norm difference:\", jnp.mean(norm_diff))\n",
    "print(\"Max norm difference:\", jnp.max(norm_diff))\n",
    "print(\"Min norm difference:\", jnp.min(norm_diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = jit(jax.vmap(jnp.linalg.norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err = norm(sample_FPI[:,:1000] - sample_LF[:,:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Description:\n",
    "    Sandbox2 for working through poster materials.\n",
    "    USE THE CORRECT ENVIRONMENT:  CHMC_FALL_2025\n",
    "\n",
    "Author: John Gallagher\n",
    "Created: 2025-09-21\n",
    "Last Modified: 2025-09-21\n",
    "Version: 1.0.0\n",
    "\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import jit\n",
    "import time\n",
    "def gauss_ndimf_jax(x, precision_matrix = None, cov=None, dim = 2):\n",
    "    \"\"\"n-Dim Gaussian target distribution.\"\"\"\n",
    "    dim = len(x)\n",
    "    # dealing with getting a precision matrix or cov matrix\n",
    "    if precision_matrix is not None and cov is not None:\n",
    "        raise MultipleMatrices(\"Please supply either a Precision Matrix or a Covariance Matrix\")\n",
    "    if precision_matrix is None and cov is not None:\n",
    "        precision_matrix = jnp.linalg.inv(cov)\n",
    "    if precision_matrix is None and cov is None:\n",
    "        precision_matrix = jnp.eye(dim)\n",
    "    return jnp.exp(-x.dot(precision_matrix.dot(x)))\n",
    "def hamiltionian(q, p, Mass_inv, target):\n",
    "    return 0.5* (p@Mass_inv@p) - jnp.log(target)\n",
    "\n",
    "target = jit(gauss_ndimf_jax)\n",
    "grad_target = jax.grad(target)\n",
    "grad_H_p = jax.grad(hamiltionian, argnums=1)\n",
    "grad_H_q = jax.grad(hamiltionian, argnums=0)\n",
    "\n",
    "\n",
    "def leapfrog(state, tau, T, Mass_inv, target_func):\n",
    "    \"Symplectic integrator: Leapfrog\"\n",
    "    N = int(T/tau)\n",
    "    q, p = state\n",
    "\n",
    "    @jit\n",
    "    def step(carry,_):\n",
    "        q, p = carry\n",
    "        q = q + 0.5*tau* grad_H_p(q, p, Mass_inv, target_func(q)) # q_half\n",
    "        p = p - tau*grad_H_q(q, p, Mass_inv, target_func(q)) # p_full\n",
    "        q = q + 0.5*tau* grad_H_p(q, p, Mass_inv, target_func(q)) # q_full\n",
    "        return [q,p], None\n",
    "\n",
    "    carry, _ = jax.lax.scan(step, [q,p], xs=None, length=N)\n",
    "    q,p=carry\n",
    "    return [q, p]\n",
    "\n",
    "@jit\n",
    "def hmc_kernel_jax(carry, key, tau, T, Mass_inv, target_func):\n",
    "    q_current, samples_history = carry\n",
    "    dim = q_current.shape[0]\n",
    "    key_draw_p, key_accept = jax.random.split(key)\n",
    "\n",
    "    # Draw momentum\n",
    "    p_current = jax.random.normal(key_draw_p, shape=(dim,))\n",
    "\n",
    "    # Integrate\n",
    "    q_star, p_star = leapfrog([q_current, p_current], tau, T, Mass_inv, target_func)\n",
    "\n",
    "    # Calculate change in Hamiltonian\n",
    "    delta_H = hamiltionian(q_star, p_star, Mass_inv, target_func(q_star)) - hamiltionian(q_current, p_current, Mass_inv, target_func(q_current))\n",
    "\n",
    "    # Acceptance step\n",
    "    alpha = jnp.minimum(1., jnp.exp(-delta_H))\n",
    "    accept = jax.random.uniform(key_accept, shape=())\n",
    "    is_accepted = accept <= alpha\n",
    "\n",
    "    q_next = jnp.where(is_accepted, q_star, q_current)\n",
    "\n",
    "    # Append accepted sample to history\n",
    "    samples_history = jnp.vstack([samples_history, q_next.reshape(1, -1)])\n",
    "\n",
    "    return (q_next, samples_history), None\n",
    "\n",
    "\n",
    "def sample_jax(initial_sample, tau, T, num_samps, key, Mass_inv, target_func):\n",
    "    \"\"\"\n",
    "    Perform Metropolis-Hastings sampling using JAX.\n",
    "    Returns:\n",
    "    jnp.ndarray: Array of sampled values.\n",
    "    \"\"\"\n",
    "    dim = initial_sample.shape[0]\n",
    "    keys = jax.random.split(key, num_samps)\n",
    "\n",
    "    # Initial carry: (current state, initial samples history)\n",
    "    initial_samples_history = initial_sample.reshape(1, -1)\n",
    "    initial_carry = (initial_sample, initial_samples_history)\n",
    "\n",
    "    # Use lax.scan for efficient sampling\n",
    "    (final_q, samples), _ = jax.lax.scan(\n",
    "        lambda carry, k: hmc_kernel_jax(carry, k, tau, T, Mass_inv, target_func),\n",
    "        initial_carry,\n",
    "        xs=keys\n",
    "    )\n",
    "\n",
    "    return samples"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
