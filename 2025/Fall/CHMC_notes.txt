John Gallagher
Oct 18, 2025
mean alpha vs dimension d, with series of different taus

Oct 17, 2025

Currently working on CHMC_tau_plots.ipynb

Updating hmc to generate jitable functions updated with appropriate statics


John Gallagher
Oct 16, 2025

Currently working on CHMC_tau_plots.ipynb

Updating hmc to generate jitable functions updated with appropriate statics



Oct 14, 2025

Some updates, validate code with matlab code
qout,pout,Jac,Reject,Iter,alpha,deltaH

To do: Diagnostic plots
) output iteration count for FPI
) deltaH histogram as function of tau
) deltaH histogram as function of tau, T
) Check acceptance rate
) Hand code jacobian and check against Jax

To Do: Project plots
MCMC inerations with 
Mu
KS Metric
Cov



Oct 10, 2025


Currently working on CHMC_with_tests.ipynb

To do: Diagnostic plots
) output iteration count for FPI
) deltaH histogram as function of tau
) deltaH histogram as function of tau, T
) Check acceptance rate
) Hand code jacobian and check against Jax

To Do: Project plots
MCMC inerations with 
Mu
KS Metric
Cov


Sept 28, 2025

Another idea: what about for a fixed radius, doing a horizion profile to look for local maxima, then using that local max to compare relative minima. 

0) grad_q_H = -\del pi(q)/pi(q) *
1) Implement AVF
2) Implement CHMC via FPI
3) Consolidate the HMC code by removing redundant experiments
4) Speed up code by removing jax.random.split() evals into a single vector

Possible to pass numpy array for p0 draws

Philosophy: If we only use jax for grad, don't re-write whole code in jax

look up multinorm determiniant for correction coefficient of gaussian

test for 

Note on jax.experimental.sparse matrix implementation in jnp, jax.scipy and scipi.sparse
https://forum.pyro.ai/t/jax-experimental-sparse-for-better-memory-speed-performance/6060/9


Some newton examples from econ:
https://jax.quantecon.org/newtons_method.html
Working Newton: 
def newton(f, x0, tol, max_iter):
    fprime = jax.grad(f)
    update = lambda x: x-f(x)/fprime(x)
    x = x0
    error = jnp.inf
    n = 0
    while error > tol:
        n +=1
        y = update(x)
        error = abs(x-y)
        x = y
    return x, n

def newton(F, x0, tol = 1e-5, max_iter=100):
    x = x0
    jacF = jax.jacobian(F)
    @jax.jit
    def update(x):
        return x - jnp.linalg.solve(jacF(x), F(x))
    error = tol+1
    n = 0
    while error > tol and n < max_iter:
        n+=1
        y = q(x)
        error = jnp.linalg.norm(y-x)
        x = y
        if n%100 ==0:
            print(f'iteration {n}, error = {error}')
    return x



Some tests on colab GPU
GPU Accelerated 10 000 dim, 
Draw sample: 0.0013899803161621094
Integration: 0.18635129928588867
1st run: 249.69273853302002
Draw sample: 0.000335693359375
Integration: 0.0001995563507080078

Draw sample: 0.0013725757598876953
Integration: 0.1289064884185791
1st run: 19.997814416885376
Draw sample: 0.00031113624572753906
Integration: 0.00019311904907226562
1000 runs:  6.723971843719482 
 1 run 0.0006723971843719482

def hmc_kernel(carry, _):
    last_sample, key = carry
    key0, key1 = jax.random.split(key)
    new_sample, _ = draw_p(last_sample, key0)
    qp_star = jit_integrator(new_sample)
    deltaH = jit_H(qp_star) - jit_H(new_sample)
    is_accepted = accept(deltaH, key0)
    qp_out = jnp.where(is_accepted, qp_star, last_sample)  # Fixed: should be last_sample, not qp
    return [qp_out, key1], _

def hmc_sampler(initial_sample, key, num_samples):
    def hmc_step(carry, _):
        last_sample, key = carry
        # Pass carry tuple to hmc_kernel, not separate arguments
        (next_sample, next_key), _ = hmc_kernel([last_sample, key], _)
        return (next_sample, next_key), next_sample
    
    (_, _), samples = jax.lax.scan(hmc_step, (initial_sample, key), xs=None, length=num_samples)
    return samples


# compile
start=time.time()
sample = hmc_sampler(key, [q0a, p0b], 1)
end = time.time()
print("1st run:", end-start)
# main run
start = time.time()
samples = hmc_sampler(key, [q0a, p0b], 100)
end = time.time()
print("1000 runs: ", end - start, "\n 1 run", (end-start)/10000)
